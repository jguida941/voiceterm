# Architecture - Visual Comparison

## ðŸ”´ CURRENT (BROKEN) FLOW

```mermaid
graph TD
    subgraph "Message 1"
        A1[User Speaks] --> B1[Spawn FFmpeg ðŸŒ]
        B1 --> C1[Spawn Whisper ðŸŒ]
        C1 --> D1[Spawn Codex ðŸŒ]
        D1 --> E1[Get Response]
        E1 --> F1[ðŸ’€ KILL EVERYTHING ðŸ’€]
    end

    subgraph "Message 2"
        A2[User Speaks Again] --> B2[Spawn FFmpeg AGAIN ðŸŒ]
        B2 --> C2[Spawn Whisper AGAIN ðŸŒ]
        C2 --> D2[Spawn Codex AGAIN ðŸŒ]
        D2 --> E2[Get Response]
        E2 --> F2[ðŸ’€ KILL EVERYTHING AGAIN ðŸ’€]
    end

    F1 -.-> A2
    Note1[Context Lost!]
    Note2[5-6 seconds each time!]
    Note3[Codex forgets everything!]
```

**Problems:**
- ðŸŒ 5-6 seconds per message
- ðŸ’€ Context destroyed every time
- ðŸ”„ Spawning 3 processes repeatedly
- ðŸ“Š No streaming (wait for full response)
- ðŸ› Terminal corruption from subprocess noise

---

## âœ… PROPOSED (CORRECT) FLOW

```mermaid
graph LR
    subgraph "Initialize Once"
        I1[Start Codex] --> I2[Load Whisper]
        I2 --> I3[Init Audio]
        I3 --> READY[âœ… Ready in <1s]
    end

    subgraph "Message Loop"
        READY --> M1[User Speaks]
        M1 --> T1[Transcribe<br/>~100ms]
        T1 --> S1[Send to Codex<br/>~10ms]
        S1 --> R1[Stream Response<br/>real-time]
        R1 --> M2[User Speaks Again]
        M2 --> T2[Transcribe<br/>~100ms]
        T2 --> S2[Send to SAME Codex<br/>~10ms]
        S2 --> R2[Stream Response<br/>real-time]
        R2 --> M3[...]
    end

    Note4[Context Preserved!]
    Note5[<200ms response time!]
    Note6[Everything stays alive!]
```

**Benefits:**
- âš¡ <200ms per message
- ðŸ§  Full context maintained
- ðŸ“¡ Real-time streaming
- ðŸŽ¯ One initialization, infinite reuse
- âœ¨ Clean terminal (no subprocess noise)

---

## ðŸ“Š Performance Comparison

| Operation | Current (Bad) | Proposed (Good) | Improvement |
|-----------|--------------|-----------------|-------------|
| FFmpeg spawn | 500ms | 0ms (pre-initialized) | â™¾ï¸ |
| Whisper load | 2000ms | 0ms (in memory) | â™¾ï¸ |
| Codex spawn | 1500ms | 0ms (persistent) | â™¾ï¸ |
| Transcription | 1000ms | 100ms (optimized) | 10x |
| Context | âŒ Lost | âœ… Preserved | Priceless |
| **Total** | **5-6 seconds** | **<200ms** | **30x faster** |

---

## ðŸ—ï¸ Architecture Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            User Interface (TUI)         â”‚ <- Ratatui + Crossterm
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Voice Service â”‚  â”‚ Input Handler  â”‚  â”‚ <- Rust async/await
â”‚  â”‚  (Whisper)   â”‚  â”‚  (Keyboard)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Session Manager                 â”‚ <- Keeps Codex alive
â”‚     (Persistent Codex Process)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Operating System (PTY)           â”‚ <- Pseudo-terminal
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ Key Insight

### Wrong Mental Model:
"Voice capture is a separate operation that needs fresh processes"

### Correct Mental Model:
"Voice is just another input method, like keyboard"

**You wouldn't restart Codex every time someone types. Why restart it when they speak?**

---

## ðŸ“ Validation Checklist

Ask any engineer these questions:

1. **Q:** Should we spawn a new process for every message?
   **A:** No, that's insane

2. **Q:** Should we keep the model loaded in memory?
   **A:** Yes, obviously

3. **Q:** Should we maintain conversation context?
   **A:** Yes, that's the whole point

4. **Q:** Is 5-6 seconds acceptable for voice input?
   **A:** No, should be near-instant

If they agree with these answers, our architecture is correct.

---

## ðŸš€ Migration Path

```
Phase 1 (2 days) - Fix Critical Issues
â”œâ”€â”€ Keep Codex session alive âœ“
â”œâ”€â”€ Fix Enter key bug âœ“
â””â”€â”€ Result: Immediate usability

Phase 2 (1 week) - Rust Rewrite
â”œâ”€â”€ Replace FFmpeg with cpal
â”œâ”€â”€ Replace subprocess Whisper with library
â”œâ”€â”€ Implement proper session management
â””â”€â”€ Result: 30x performance gain

Phase 3 (3-4 days) - Advanced Features
â”œâ”€â”€ Voice Activity Detection
â”œâ”€â”€ Wake word ("Hey Codex")
â”œâ”€â”€ Streaming transcription
â””â”€â”€ Result: Professional voice assistant

Phase 4 (2-3 days) - Distribution
â”œâ”€â”€ Single binary compilation
â”œâ”€â”€ Homebrew formula
â”œâ”€â”€ Cross-platform support
â””â”€â”€ Result: Easy installation
```

---

## ðŸŽª The Bottom Line

**Current approach** = Like restarting your computer to open each email
**Proposed approach** = Like keeping email app open

Which one makes more sense?